@misc{oversquashing,
      title={On the Bottleneck of Graph Neural Networks and its Practical Implications}, 
      author={Uri Alon and Eran Yahav},
      year={2021},
      eprint={2006.05205},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.05205}, 
}
@inproceedings{LRGB,
  title={Long Range Graph Benchmark},
  author={Dwivedi, Vijay Prakash and Rampášek, Ladislav and Galkin, Mikhail and
          Sanyal, Shagun and Luan, Yecheng and Khan, Soumya and
          Zhang, Beidi and Zhang, Muhan and Hooi, Bryan and
          Günnemann, Stephan and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}
@article{pyg,
  title={{PyG 2.0}: Scalable Learning on Real World Graphs},
  author={Fey, Matthias and Sunil, Jinu and Nitta, Akihiro and Puri, Rishi and Shah, Manan and Stojanovi{\'c}, Bla{\v{z}} and Rozemberczki, Benedek},
  journal={arXiv preprint arXiv:2110.02905},
  year={2021},
  url={https://arxiv.org},
}
@Inbook{earlystopping,
author="Prechelt, Lutz",
title="Early Stopping --- But When?",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="53--67",
abstract="Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (``early stopping''). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4{\%} on average), but cost much more training time (here: about factor 4 longer on average).",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_5",
url="https://doi.org/10.1007/978-3-642-35289-8_5"
}
@inproceedings{position,
title={Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks},
author={Maya Bechler-Speicher and Ben Finkelshtein and Fabrizio Frasca and Luis M{\"u}ller and Jan T{\"o}nshoff and Antoine Siraudin and Viktor Zaverkin and Michael M. Bronstein and Mathias Niepert and Bryan Perozzi and Mikhail Galkin and Christopher Morris},
booktitle={Forty-second International Conference on Machine Learning Position Paper Track},
year={2025},
url={https://openreview.net/forum?id=nDFpl2lhoH}
}
@misc{egp,
      title={Expander Graph Propagation}, 
      author={Andreea Deac and Marc Lackenby and Petar Veličković},
      year={2022},
      eprint={2210.02997},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.02997}, 
}
@misc{cgp,
      title={Cayley Graph Propagation}, 
      author={JJ Wilson and Maya Bechler-Speicher and Petar Veličković},
      year={2025},
      eprint={2410.03424},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.03424}, 
}
@book{expander,
  title={Expander families and Cayley graphs: a beginner's guide},
  author={Krebs, Mike and Shaheen, Anthony},
  year={2011},
  publisher={Oxford University Press}
}
@article{virtual_nodes,
  title={Understanding virtual nodes: Oversquashing and node heterogeneity},
  author={Southern, Joshua and Di Giovanni, Francesco and Bronstein, Michael and Lutzeyer, Johannes F},
  journal={arXiv preprint arXiv:2405.13526},
  year={2024}
}
@article{gin,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}